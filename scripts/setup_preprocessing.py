#!/usr/bin/env python3
"""
Setup script for Persian text preprocessing components
Validates dependencies and prepares external resources
"""

import sys
import subprocess
from pathlib import Path
import json
import requests
from typing import List


def check_hazm_installation():
    """Check if hazm is properly installed"""
    try:
        import hazm
        print("âœ… Hazm library installed successfully")

        # Test basic functionality
        normalizer = hazm.Normalizer()
        test_text = "Ø³Ù„Ø§Ù…! Ø§ÛŒÙ† ÛŒÚ© Ù…ØªÙ† ØªØ³Øª Ø§Ø³Øª."
        normalized = normalizer.normalize(test_text)
        print(f"âœ… Hazm normalization test passed")

        return True
    except ImportError:
        print("âŒ Hazm library not found")
        return False
    except Exception as e:
        print(f"âš ï¸ Hazm installation issue: {e}")
        return False


def install_hazm():
    """Install hazm library"""
    print("ğŸ“¦ Installing hazm library...")
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", "hazm"])
        print("âœ… Hazm installed successfully")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ Failed to install hazm: {e}")
        return False


def download_persian_stopwords():
    """Download or create Persian stopwords list"""
    stopwords_file = Path("data/external/persian_stopwords.txt")

    if stopwords_file.exists():
        print("âœ… Persian stopwords file already exists")
        return True

    # Create comprehensive Persian stopwords list
    persian_stopwords = [
        # Common Persian stopwords
        "Ø¯Ø±", "Ø¨Ù‡", "Ø§Ø²", "Ú©Ù‡", "Ø§ÛŒÙ†", "Ø¢Ù†", "Ø±Ø§", "Ø¨Ø§", "ØªØ§", "Ø¨Ø±Ø§ÛŒ",
        "Ùˆ", "ÛŒØ§", "Ø§Ù…Ø§", "ÙˆÙ„ÛŒ", "Ú†ÙˆÙ†", "Ø§Ú¯Ø±", "ÙˆÙ‚ØªÛŒ", "Ø²Ù…Ø§Ù†ÛŒ", "Ù‡Ù†Ú¯Ø§Ù…ÛŒ",
        "Ø§Ø³Øª", "Ø¨ÙˆØ¯", "Ø¨Ø§Ø´Ø¯", "Ø´Ø¯", "Ù…ÛŒ", "Ø®ÙˆØ§Ù‡Ø¯", "Ú©Ø±Ø¯", "Ù†Ù…ÛŒ", "Ù†Ø®ÙˆØ§Ù‡Ø¯",
        "Ù…Ù†", "ØªÙˆ", "Ø§Ùˆ", "Ù…Ø§", "Ø´Ù…Ø§", "Ø¢Ù†Ù‡Ø§", "Ø®ÙˆØ¯", "Ø®ÙˆØ¯Ù…", "Ø®ÙˆØ¯Øª", "Ø®ÙˆØ¯Ø´",
        "Ù‡Ø±", "Ù‡Ù…Ù‡", "Ø¨Ø¹Ø¶ÛŒ", "Ú†Ù†Ø¯", "ÛŒÚ©", "Ø¯Ùˆ", "Ø³Ù‡", "Ú†Ù‡Ø§Ø±", "Ù¾Ù†Ø¬",
        "Ø±ÙˆÛŒ", "Ø²ÛŒØ±", "Ú©Ù†Ø§Ø±", "Ù†Ø²Ø¯ÛŒÚ©", "Ø¯ÙˆØ±", "Ø¬Ù„Ùˆ", "Ø¹Ù‚Ø¨", "Ø¨Ø§Ù„Ø§", "Ù¾Ø§ÛŒÛŒÙ†",
        "Ø®ÛŒÙ„ÛŒ", "Ø¨Ø³ÛŒØ§Ø±", "Ú©Ù…ÛŒ", "Ø§Ù†Ø¯Ú©ÛŒ", "Ø²ÛŒØ§Ø¯", "Ú©Ù…", "ÙÙ‚Ø·", "Ù‡Ù…", "Ù†ÛŒØ²",
        "Ø§Ù„Ø¨ØªÙ‡", "Ù…Ø·Ù…Ø¦Ù†Ø§Ù‹", "Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹", "Ø´Ø§ÛŒØ¯", "Ø¨Ø§ÛŒØ¯", "Ù†Ø¨Ø§ÛŒØ¯", "Ø¨Ø§Ø²", "Ø¯ÙˆØ¨Ø§Ø±Ù‡",

        # Banking and app specific stopwords
        "Ø¨Ø§Ù†Ú©", "Ø§Ù¾", "Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù†", "Ù…ÙˆØ¨Ø§ÛŒÙ„", "Ø³ÛŒØ³ØªÙ…", "ÙˆØ¨", "Ø³Ø§ÛŒØª",
        "Ø¨Ø±Ù†Ø§Ù…Ù‡", "Ù†Ø±Ù… Ø§ÙØ²Ø§Ø±", "Ø¯Ø§Ù†Ù„ÙˆØ¯", "Ù†ØµØ¨", "Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ", "Ø¢Ù¾Ø¯ÛŒØª",
        "Ú©Ø§Ø±Øª", "Ø±Ù…Ø²", "Ù¾Ø³ÙˆØ±Ø¯", "Ø­Ø³Ø§Ø¨", "Ø§Ú©Ø§Ù†Øª", "Ù„Ø§Ú¯ÛŒÙ†", "ÙˆØ±ÙˆØ¯", "Ø®Ø±ÙˆØ¬",

        # Common filler words
        "Ø¢Ø±Ù‡", "Ø¨Ù„Ù‡", "Ù†Ù‡", "Ø®Ø¨", "Ø§ÙˆÚ©ÛŒ", "Ø¨Ø§Ø´Ù‡", "Ú†Ø´Ù…", "Ù…Ø±Ø³ÛŒ", "Ù…Ù…Ù†ÙˆÙ†",
        "Ø³Ù„Ø§Ù…", "Ø®Ø¯Ø§Ø­Ø§ÙØ¸", "Ø¯Ø±Ø³Øª", "ØºÙ„Ø·", "ØµØ­ÛŒØ­", "Ù†Ø§Ø¯Ø±Ø³Øª"
    ]

    # Ensure directory exists
    stopwords_file.parent.mkdir(parents=True, exist_ok=True)

    # Write stopwords to file
    with open(stopwords_file, 'w', encoding='utf-8') as f:
        for word in persian_stopwords:
            f.write(word + '\n')

    print(
        f"âœ… Created Persian stopwords file with {len(persian_stopwords)} words")
    return True


def validate_emoji_mapping():
    """Validate emoji mapping file"""
    emoji_file = Path("data/external/persian_emoji_mapping.json")

    if not emoji_file.exists():
        print("âŒ Emoji mapping file not found")
        return False

    try:
        with open(emoji_file, 'r', encoding='utf-8') as f:
            emoji_data = json.load(f)

        # Count total emojis
        total_emojis = 0
        for category in emoji_data.values():
            if isinstance(category, dict):
                total_emojis += len(category)

        print(f"âœ… Emoji mapping file validated with {total_emojis} emojis")
        return True

    except Exception as e:
        print(f"âŒ Error validating emoji mapping: {e}")
        return False


def test_preprocessing_pipeline():
    """Test the preprocessing pipeline with sample text"""
    try:
        # Add project root to path
        project_root = Path(__file__).parent.parent
        sys.path.append(str(project_root))

        from src.preprocessing.persian_cleaner import PersianTextCleaner

        # Initialize cleaner
        cleaner = PersianTextCleaner()

        # Test texts
        test_texts = [
            "Ø³Ù„Ø§Ù…! Ø§Ù¾ Ø¨Ø§Ù†Ú© Ø¹Ø§Ù„ÛŒÙ‡ ğŸ˜Š Ø®ÛŒÙ„ÛŒ Ø±Ø§Ø­Øª Ú©Ø§Ø± Ù…ÛŒÚ©Ù†Ù‡",
            "Ø§ÛŒÙ† Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø®Ø±Ø§Ø¨ Ø§Ø³Øª ğŸ˜¡ Ù…Ø´Ú©Ù„ Ø¯Ø§Ø±Ù‡",
            "www.example.com Ø´Ù…Ø§Ø±Ù‡ ØªÙ…Ø§Ø³: 09123456789",
            "Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø± Ø®ÙˆØ¨ÛŒ Ø§Ø³Øª ÙˆÙ„ÛŒ Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ØªØ± Ø¨Ø´Ù‡"
        ]

        print("ğŸ§ª Testing preprocessing pipeline...")

        for i, text in enumerate(test_texts, 1):
            try:
                # Test different cleaning levels
                light = cleaner.clean_text(text, level='light')
                medium = cleaner.clean_text(text, level='medium')
                heavy = cleaner.clean_text(text, level='heavy')

                # Test tokenization
                tokens = cleaner.tokenize(medium)

                print(f"âœ… Test {i} passed")

            except Exception as e:
                print(f"âŒ Test {i} failed: {e}")
                return False

        # Get statistics
        stats = cleaner.get_statistics()
        print(
            f"âœ… Pipeline test completed. Processed {stats['processing_stats']['processed_texts']} texts")

        return True

    except Exception as e:
        print(f"âŒ Pipeline test failed: {e}")
        return False


def create_sample_data():
    """Create sample data for testing"""
    sample_file = Path("data/raw/sample_comments.csv")

    if sample_file.exists():
        print("âœ… Sample data file already exists")
        return True

    # Sample Persian banking comments
    sample_data = [
        {
            'id': 1,
            'app_name': 'Test Bank',
            'comment': 'Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù† Ø¹Ø§Ù„ÛŒ Ø§Ø³Øª! ğŸ˜Š Ø®ÛŒÙ„ÛŒ Ø±Ø§Ø­Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯',
            'rating': 5
        },
        {
            'id': 2,
            'app_name': 'Test Bank',
            'comment': 'Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù…Ø´Ú©Ù„ Ø¯Ø§Ø±Ø¯ ğŸ˜  Ù†Ù…ÛŒâ€ŒØªÙˆÙ†Ù… ÙˆØ§Ø±Ø¯ Ø¨Ø´Ù…',
            'rating': 1
        },
        {
            'id': 3,
            'app_name': 'Another Bank',
            'comment': 'Ø³Ø±Ø¹Øª Ø®ÙˆØ¨ÛŒ Ø¯Ø§Ø±Ù‡ ÙˆÙ„ÛŒ Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø¨Ù‡ØªØ± Ø¨Ø§Ø´Ù‡ Ø¨Ù‡ØªØ±Ù‡',
            'rating': 3
        },
        {
            'id': 4,
            'app_name': 'Test Bank',
            'comment': 'Ú©Ø§Ø±Øª Ø¨Ù‡ Ú©Ø§Ø±Øª Ø±Ø§Ø­Øª Ø´Ø¯Ù‡ ğŸ‘ Ù…Ù…Ù†ÙˆÙ† Ø§Ø² ØªÛŒÙ… Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù†ÙˆÛŒØ³ÛŒ',
            'rating': 4
        },
        {
            'id': 5,
            'app_name': 'Another Bank',
            'comment': 'Ú†Ø±Ø§ Ø§ÛŒÙ†Ù‚Ø¯Ø± Ú©Ù†Ø¯ Ø§Ø³ØªØŸ Ù„Ø·ÙØ§Ù‹ Ø¨Ù‡Ø´ Ø±Ø³ÛŒØ¯Ú¯ÛŒ Ú©Ù†ÛŒØ¯',
            'rating': 2
        }
    ]

    import pandas as pd

    # Ensure directory exists
    sample_file.parent.mkdir(parents=True, exist_ok=True)

    # Create DataFrame and save
    df = pd.DataFrame(sample_data)
    df.to_csv(sample_file, index=False, encoding='utf-8')

    print(f"âœ… Created sample data file with {len(sample_data)} comments")
    return True


def main():
    """Main setup function"""
    print("ğŸš€ Persian Text Preprocessing Setup")
    print("=" * 50)

    success = True

    # Check Python version
    if sys.version_info < (3, 8):
        print("âŒ Python 3.8 or higher is required")
        success = False
    else:
        print(f"âœ… Python {sys.version.split()[0]} detected")

    print("\nğŸ“¦ Checking Dependencies")
    print("-" * 30)

    # Check hazm installation
    if not check_hazm_installation():
        install_choice = input("Install hazm library? (y/N): ").strip().lower()
        if install_choice == 'y':
            if not install_hazm():
                success = False
        else:
            print("âš ï¸ Hazm is required for optimal Persian text processing")

    print("\nğŸ“ Setting up External Resources")
    print("-" * 30)

    # Setup stopwords
    if not download_persian_stopwords():
        success = False

    # Validate emoji mapping
    if not validate_emoji_mapping():
        print("âš ï¸ Emoji mapping file missing. Please ensure it exists at data/external/persian_emoji_mapping.json")

    print("\nğŸ§ª Testing Components")
    print("-" * 30)

    # Test preprocessing pipeline
    if not test_preprocessing_pipeline():
        success = False

    print("\nğŸ“ Creating Sample Data")
    print("-" * 30)

    # Create sample data for testing
    if not create_sample_data():
        success = False

    print("\n" + "=" * 50)

    if success:
        print("ğŸ‰ Setup completed successfully!")
        print("\nNext steps:")
        print("1. Run: python scripts/preprocessing_pipeline.py --input data/raw/sample_comments.csv")
        print("2. Check results in data/processed/")
        print("3. Analyze results with notebooks/02_preprocessing_analysis.ipynb")
    else:
        print("âš ï¸ Setup completed with warnings")
        print("Please resolve the issues above before proceeding")

    print(f"\nFor detailed preprocessing, run:")
    print(f"python scripts/preprocessing_pipeline.py --help")


if __name__ == "__main__":
    main()
